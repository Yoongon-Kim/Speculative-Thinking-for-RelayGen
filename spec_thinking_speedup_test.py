"""
Speculative Thinking Speedup Benchmark

This script measures the speedup of speculative thinking (Qwen3-1.7B + Qwen3-32B)
compared to using the large model (Qwen3-32B) alone on AIME25 dataset problems.

Metrics measured:
- Wall clock time (latency)
- Token throughput (tokens/second)
- Large model token ratio (percentage of tokens generated by the large model)
- Total tokens generated

The test runs on the first 5 problems from AIME25 dataset, with 5 runs per problem.
Both models are loaded simultaneously on 2 GPUs to fit memory constraints.
"""

import os
import sys
import time
import json
import argparse
import numpy as np
from typing import List, Dict, Any, Tuple
from datetime import datetime
from datasets import load_dataset
import torch
import ray
import yaml

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from speculative.speculative_vllm import spe_thinking_vllm
from speculative.speculative_vllm_2gpu import spe_thinking_vllm_2gpu
from utils.data_utils import read_yml
from transformers import AutoTokenizer


class SpeedupBenchmark:
    """Benchmark class to measure speculative thinking speedup."""

    def __init__(self, config_path: str, num_problems: int = 5, num_runs: int = 5,
                 temperature: float = 0.6, top_p: float = 0.95, top_k: int = 20,
                 max_tokens: int = 32768,
                 test_mode: str = "both",
                 use_2gpu: bool = False,
                 aime_config_path: str = "skythought_evals/tasks/aime/aime25.yaml"):
        """
        Initialize the benchmark.

        Args:
            config_path: Path to speculative thinking config file
            num_problems: Number of AIME25 problems to test (default: 5)
            num_runs: Number of runs per problem (default: 5)
            temperature: Sampling temperature (default: 0.6)
            top_p: Nucleus sampling parameter (default: 0.95)
            top_k: Top-k sampling parameter (default: 20)
            max_tokens: Maximum tokens to generate (default: 32768)
            test_mode: Which test to run - "large", "spec", or "both" (default: both)
            use_2gpu: Use 2-GPU optimized version (loads both models in same process) (default: False)
            aime_config_path: Path to AIME25 task config (default: skythought_evals/tasks/aime/aime25.yaml)
        """
        self.config_path = config_path
        self.num_problems = num_problems
        self.num_runs = num_runs
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.max_tokens = max_tokens
        self.test_mode = test_mode.lower()
        self.use_2gpu = use_2gpu
        self.config = read_yml(config_path)

        # Validate test mode
        if self.test_mode not in ["large", "spec", "both"]:
            raise ValueError(f"Invalid test_mode '{test_mode}'. Must be 'large', 'spec', or 'both'.")

        # Load AIME25 task config for prompt template
        self.aime_config = self.load_aime_config(aime_config_path)
        self.prompt_template = self.aime_config['templating_parameters']['template']

        # Initialize Ray if not already initialized
        if not ray.is_initialized():
            ray.init(ignore_reinit_error=True)

        print("Loading AIME25 dataset...")
        self.problems = self.load_aime25_problems()
        print(f"Loaded {len(self.problems)} problems from AIME25 dataset\n")

    def load_aime_config(self, config_path: str) -> Dict[str, Any]:
        """Load AIME25 task configuration from YAML file."""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def load_aime25_problems(self) -> List[Dict[str, str]]:
        """Load the first N problems from AIME25 dataset."""
        dataset = load_dataset("yentinglin/aime_2025", split="train")

        problems = []
        for i in range(min(self.num_problems, len(dataset))):
            problem = dataset[i]
            # Format using template from AIME25 config
            prompt = self.prompt_template.format(prompt=problem['problem'])
            problems.append({
                'id': i,
                'problem': problem['problem'],
                'prompt': prompt,
                'answer': problem.get('answer', 'N/A')
            })

        return problems

    def create_messages(self, prompt: str) -> List[Dict[str, str]]:
        """Create messages format for model generation."""
        return [{"role": "user", "content": prompt}]

    def test_large_model_only(self) -> Dict[str, Any]:
        """
        Test using only the large model (Qwen3-32B).
        Returns metrics for all problems and runs.
        """
        print("="*80)
        print("TESTING LARGE MODEL ONLY (Qwen3-32B)")
        print("="*80)

        # Create config for large model only (no speculative model)
        large_model_config = self.config.copy()
        large_model_config['speculative_model_name'] = None

        print(f"Initializing large model on {large_model_config['target_model_gpu']} GPUs...")
        model = spe_thinking_vllm(**large_model_config)
        print("Model initialized successfully")

        # Warmup run to initialize CUDA kernels and KV-cache
        print("\nPerforming warmup run to initialize CUDA kernels...")
        warmup_messages = [{"role": "user", "content": "What is 2+2?"}]
        _ = model.generate(
            messages=warmup_messages,
            max_tokens=50,
            temperature=self.temperature,
            top_k=self.top_k,
            top_p=self.top_p
        )
        print("Warmup complete\n")

        results = {
            'model': 'Qwen3-32B (Large Only)',
            'problems': []
        }

        for problem in self.problems:
            print(f"\nProblem {problem['id'] + 1}/{self.num_problems}")
            print(f"Problem: {problem['problem'][:100]}...")

            problem_results = {
                'id': problem['id'],
                'problem': problem['problem'],
                'runs': []
            }

            for run in range(self.num_runs):
                print(f"  Run {run + 1}/{self.num_runs}...", end=" ", flush=True)

                messages = self.create_messages(problem['prompt'])

                # Measure generation
                start_time = time.time()
                generated_text, num_tokens, _, _ = model.generate(
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    top_p=self.top_p
                )
                end_time = time.time()

                latency = end_time - start_time
                throughput = num_tokens / latency if latency > 0 else 0

                run_result = {
                    'run': run,
                    'latency': latency,
                    'num_tokens': num_tokens,
                    'throughput': throughput,
                    'large_model_ratio': 1.0,  # 100% for large model only
                    'generated_text': generated_text
                }

                problem_results['runs'].append(run_result)
                print(f"Latency: {latency:.2f}s, Tokens: {num_tokens}, Throughput: {throughput:.2f} tok/s")

            results['problems'].append(problem_results)

        # Clean up
        del model
        ray.shutdown()
        time.sleep(2)  # Wait for resources to be freed
        if not ray.is_initialized():
            ray.init(ignore_reinit_error=True)

        return results

    def test_speculative_thinking(self) -> Dict[str, Any]:
        """
        Test using speculative thinking (Qwen3-1.7B + Qwen3-32B).
        Returns metrics for all problems and runs.
        """
        print("\n" + "="*80)
        print("TESTING SPECULATIVE THINKING (Qwen3-1.7B + Qwen3-32B)")
        print("="*80)

        if self.use_2gpu:
            print("Using 2-GPU optimized version (both models in same process)")
            print(f"  Both models will use 2 GPUs with tensor parallelism")
            model = spe_thinking_vllm_2gpu(**self.config)
        else:
            print(f"Using standard Ray actor version")
            print(f"  Target model (Qwen3-32B): {self.config['target_model_gpu']} GPUs")
            print(f"  Speculative model (Qwen3-1.7B): {self.config['speculative_model_gpu']} GPU(s)")
            model = spe_thinking_vllm(**self.config)

        print("Models initialized successfully")

        # Warmup run to initialize CUDA kernels and KV-cache
        print("\nPerforming warmup run to initialize CUDA kernels...")
        warmup_messages = [{"role": "user", "content": "What is 2+2?"}]
        _ = model.generate(
            messages=warmup_messages,
            max_tokens=50,
            temperature=self.temperature,
            top_k=self.top_k,
            top_p=self.top_p
        )
        print("Warmup complete\n")

        results = {
            'model': 'Speculative Thinking (Qwen3-1.7B + Qwen3-32B)',
            'problems': []
        }

        for problem in self.problems:
            print(f"\nProblem {problem['id'] + 1}/{self.num_problems}")
            print(f"Problem: {problem['problem'][:100]}...")

            problem_results = {
                'id': problem['id'],
                'problem': problem['problem'],
                'runs': []
            }

            for run in range(self.num_runs):
                print(f"  Run {run + 1}/{self.num_runs}...", end=" ", flush=True)

                messages = self.create_messages(problem['prompt'])

                # Measure generation
                start_time = time.time()
                generated_text, num_tokens, correct_tokens, try_correct_num = model.generate(
                    messages=messages,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    top_p=self.top_p
                )
                end_time = time.time()

                latency = end_time - start_time
                throughput = num_tokens / latency if latency > 0 else 0

                # Calculate large model token ratio
                large_model_tokens = sum(ct['token_num'] for ct in correct_tokens)
                large_model_ratio = large_model_tokens / num_tokens if num_tokens > 0 else 0

                run_result = {
                    'run': run,
                    'latency': latency,
                    'num_tokens': num_tokens,
                    'throughput': throughput,
                    'large_model_tokens': large_model_tokens,
                    'large_model_ratio': large_model_ratio,
                    'num_validations': try_correct_num,
                    'generated_text': generated_text
                }

                problem_results['runs'].append(run_result)
                print(f"Latency: {latency:.2f}s, Tokens: {num_tokens}, "
                      f"Throughput: {throughput:.2f} tok/s, Large ratio: {large_model_ratio:.2%}")

            results['problems'].append(problem_results)

        # Clean up
        del model
        ray.shutdown()

        return results

    def calculate_statistics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate statistics from benchmark results."""
        all_latencies = []
        all_throughputs = []
        all_tokens = []
        all_large_ratios = []

        for problem in results['problems']:
            for run in problem['runs']:
                all_latencies.append(run['latency'])
                all_throughputs.append(run['throughput'])
                all_tokens.append(run['num_tokens'])
                all_large_ratios.append(run['large_model_ratio'])

        stats = {
            'latency': {
                'mean': np.mean(all_latencies),
                'std': np.std(all_latencies),
                'min': np.min(all_latencies),
                'max': np.max(all_latencies),
                'median': np.median(all_latencies)
            },
            'throughput': {
                'mean': np.mean(all_throughputs),
                'std': np.std(all_throughputs),
                'min': np.min(all_throughputs),
                'max': np.max(all_throughputs),
                'median': np.median(all_throughputs)
            },
            'tokens': {
                'mean': np.mean(all_tokens),
                'std': np.std(all_tokens),
                'min': np.min(all_tokens),
                'max': np.max(all_tokens),
                'median': np.median(all_tokens)
            },
            'large_model_ratio': {
                'mean': np.mean(all_large_ratios),
                'std': np.std(all_large_ratios),
                'min': np.min(all_large_ratios),
                'max': np.max(all_large_ratios),
                'median': np.median(all_large_ratios)
            }
        }

        return stats

    def print_comparison(self, large_stats: Dict[str, Any], spec_stats: Dict[str, Any]):
        """Print comparison between large model and speculative thinking."""
        print("\n" + "="*80)
        print("BENCHMARK RESULTS SUMMARY")
        print("="*80)

        print("\n1. LATENCY (seconds)")
        print("-" * 80)
        print(f"{'Metric':<20} {'Large Model':<20} {'Spec Thinking':<20} {'Speedup':<20}")
        print("-" * 80)

        speedup = large_stats['latency']['mean'] / spec_stats['latency']['mean']
        print(f"{'Mean':<20} {large_stats['latency']['mean']:>18.2f}s {spec_stats['latency']['mean']:>18.2f}s {speedup:>18.2f}x")
        print(f"{'Median':<20} {large_stats['latency']['median']:>18.2f}s {spec_stats['latency']['median']:>18.2f}s {large_stats['latency']['median']/spec_stats['latency']['median']:>18.2f}x")
        print(f"{'Std Dev':<20} {large_stats['latency']['std']:>18.2f}s {spec_stats['latency']['std']:>18.2f}s")
        print(f"{'Min':<20} {large_stats['latency']['min']:>18.2f}s {spec_stats['latency']['min']:>18.2f}s")
        print(f"{'Max':<20} {large_stats['latency']['max']:>18.2f}s {spec_stats['latency']['max']:>18.2f}s")

        print("\n2. THROUGHPUT (tokens/second)")
        print("-" * 80)
        print(f"{'Metric':<20} {'Large Model':<20} {'Spec Thinking':<20} {'Improvement':<20}")
        print("-" * 80)

        throughput_improvement = spec_stats['throughput']['mean'] / large_stats['throughput']['mean']
        print(f"{'Mean':<20} {large_stats['throughput']['mean']:>16.2f} t/s {spec_stats['throughput']['mean']:>16.2f} t/s {throughput_improvement:>18.2f}x")
        print(f"{'Median':<20} {large_stats['throughput']['median']:>16.2f} t/s {spec_stats['throughput']['median']:>16.2f} t/s {spec_stats['throughput']['median']/large_stats['throughput']['median']:>18.2f}x")
        print(f"{'Std Dev':<20} {large_stats['throughput']['std']:>16.2f} t/s {spec_stats['throughput']['std']:>16.2f} t/s")
        print(f"{'Min':<20} {large_stats['throughput']['min']:>16.2f} t/s {spec_stats['throughput']['min']:>16.2f} t/s")
        print(f"{'Max':<20} {large_stats['throughput']['max']:>16.2f} t/s {spec_stats['throughput']['max']:>16.2f} t/s")

        print("\n3. TOKEN GENERATION")
        print("-" * 80)
        print(f"{'Metric':<20} {'Large Model':<20} {'Spec Thinking':<20}")
        print("-" * 80)
        print(f"{'Mean Tokens':<20} {large_stats['tokens']['mean']:>18.0f} {spec_stats['tokens']['mean']:>18.0f}")
        print(f"{'Median Tokens':<20} {large_stats['tokens']['median']:>18.0f} {spec_stats['tokens']['median']:>18.0f}")

        print("\n4. LARGE MODEL USAGE RATIO")
        print("-" * 80)
        print(f"{'Metric':<20} {'Large Model':<20} {'Spec Thinking':<20}")
        print("-" * 80)
        print(f"{'Mean Ratio':<20} {large_stats['large_model_ratio']['mean']:>17.1%} {spec_stats['large_model_ratio']['mean']:>17.1%}")
        print(f"{'Median Ratio':<20} {large_stats['large_model_ratio']['median']:>17.1%} {spec_stats['large_model_ratio']['median']:>17.1%}")

        print("\n" + "="*80)
        print(f"OVERALL SPEEDUP: {speedup:.2f}x")
        print(f"THROUGHPUT IMPROVEMENT: {throughput_improvement:.2f}x")
        print(f"LARGE MODEL TOKEN REDUCTION: {(1 - spec_stats['large_model_ratio']['mean']) * 100:.1f}%")
        print("="*80)

    def save_results(self, large_results: Dict[str, Any], spec_results: Dict[str, Any],
                     large_stats: Dict[str, Any], spec_stats: Dict[str, Any]):
        """Save detailed results to JSON file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "benchmark_results"
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"speedup_benchmark_{timestamp}.json")

        # Convert numpy types to native Python types for JSON serialization
        def convert_to_native(obj):
            """Recursively convert numpy types to Python native types."""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_to_native(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_native(item) for item in obj]
            else:
                return obj

        results = {
            'timestamp': timestamp,
            'config': {
                'num_problems': self.num_problems,
                'num_runs': self.num_runs,
                'config_path': self.config_path,
                'target_model': self.config['target_model_name'],
                'speculative_model': self.config['speculative_model_name'],
                'target_gpus': self.config['target_model_gpu'],
                'speculative_gpus': self.config['speculative_model_gpu']
            },
            'large_model': {
                'results': convert_to_native(large_results),
                'statistics': convert_to_native(large_stats)
            },
            'speculative_thinking': {
                'results': convert_to_native(spec_results),
                'statistics': convert_to_native(spec_stats)
            },
            'comparison': {
                'speedup': float(large_stats['latency']['mean'] / spec_stats['latency']['mean']),
                'throughput_improvement': float(spec_stats['throughput']['mean'] / large_stats['throughput']['mean']),
                'large_model_reduction': float(1 - spec_stats['large_model_ratio']['mean'])
            }
        }

        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"\nDetailed results saved to: {output_file}")

    def print_single_model_stats(self, model_name: str, stats: Dict[str, Any]):
        """Print statistics for a single model."""
        print(f"\n{model_name} Statistics:")
        print("-" * 80)

        print("\n1. LATENCY (seconds)")
        print(f"  Mean:   {stats['latency']['mean']:>10.2f}s")
        print(f"  Median: {stats['latency']['median']:>10.2f}s")
        print(f"  Std:    {stats['latency']['std']:>10.2f}s")
        print(f"  Min:    {stats['latency']['min']:>10.2f}s")
        print(f"  Max:    {stats['latency']['max']:>10.2f}s")

        print("\n2. THROUGHPUT (tokens/second)")
        print(f"  Mean:   {stats['throughput']['mean']:>10.2f} t/s")
        print(f"  Median: {stats['throughput']['median']:>10.2f} t/s")
        print(f"  Std:    {stats['throughput']['std']:>10.2f} t/s")
        print(f"  Min:    {stats['throughput']['min']:>10.2f} t/s")
        print(f"  Max:    {stats['throughput']['max']:>10.2f} t/s")

        print("\n3. TOKEN GENERATION")
        print(f"  Mean tokens:   {stats['tokens']['mean']:>10.0f}")
        print(f"  Median tokens: {stats['tokens']['median']:>10.0f}")

        if 'large_model_ratio' in stats:
            print("\n4. LARGE MODEL USAGE RATIO")
            print(f"  Mean ratio:   {stats['large_model_ratio']['mean']:>9.1%}")
            print(f"  Median ratio: {stats['large_model_ratio']['median']:>9.1%}")

    def save_single_model_results(self, model_type: str, results: Dict[str, Any],
                                   stats: Dict[str, Any]):
        """Save results for a single model to JSON file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "benchmark_results"
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"{model_type}_benchmark_{timestamp}.json")

        # Convert numpy types to native Python types for JSON serialization
        def convert_to_native(obj):
            """Recursively convert numpy types to Python native types."""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_to_native(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_native(item) for item in obj]
            else:
                return obj

        output = {
            'timestamp': timestamp,
            'model_type': model_type,
            'config': {
                'num_problems': self.num_problems,
                'num_runs': self.num_runs,
                'config_path': self.config_path,
                'target_model': self.config['target_model_name'],
                'speculative_model': self.config.get('speculative_model_name', None),
                'temperature': self.temperature,
                'top_p': self.top_p,
                'top_k': self.top_k,
                'max_tokens': self.max_tokens,
            },
            'results': convert_to_native(results),
            'statistics': convert_to_native(stats)
        }

        with open(output_file, 'w') as f:
            json.dump(output, f, indent=2)

        print(f"\nResults saved to: {output_file}")

    def run(self):
        """Run the complete benchmark."""
        print("\n" + "="*80)
        print("SPECULATIVE THINKING SPEEDUP BENCHMARK")
        print("="*80)
        print(f"Dataset: AIME25 (first {self.num_problems} problems)")
        print(f"Runs per problem: {self.num_runs}")
        print(f"Test mode: {self.test_mode}")
        print(f"Config: {self.config_path}")
        print("="*80 + "\n")

        large_results = None
        large_stats = None
        spec_results = None
        spec_stats = None

        # Run tests based on mode
        if self.test_mode in ["large", "both"]:
            large_results = self.test_large_model_only()
            large_stats = self.calculate_statistics(large_results)

            # Aggressive cleanup before next test
            if self.test_mode == "both":
                print("\n" + "="*80)
                print("CLEANING UP RESOURCES BEFORE SPECULATIVE THINKING TEST")
                print("="*80)
                import gc
                import torch
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                print("GPU memory cleared")
                time.sleep(5)  # Wait for resources to be fully released
                print()

        if self.test_mode in ["spec", "both"]:
            spec_results = self.test_speculative_thinking()
            spec_stats = self.calculate_statistics(spec_results)

        # Print comparison or individual results
        if self.test_mode == "both":
            self.print_comparison(large_stats, spec_stats)
            self.save_results(large_results, spec_results, large_stats, spec_stats)
        elif self.test_mode == "large":
            print("\n" + "="*80)
            print("LARGE MODEL ONLY RESULTS")
            print("="*80)
            self.print_single_model_stats("Large Model (Qwen3-32B)", large_stats)
            self.save_single_model_results("large_model", large_results, large_stats)
        elif self.test_mode == "spec":
            print("\n" + "="*80)
            print("SPECULATIVE THINKING RESULTS")
            print("="*80)
            self.print_single_model_stats("Speculative Thinking (Qwen3-1.7B + Qwen3-32B)", spec_stats)
            self.save_single_model_results("speculative_thinking", spec_results, spec_stats)

        print("\nBenchmark completed successfully!")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Benchmark speculative thinking speedup on AIME25 dataset"
    )

    # Model and configuration
    parser.add_argument(
        "--config",
        type=str,
        default="speculative/config/qwen3_1.7b_32b.yml",
        help="Path to speculative thinking config file (default: speculative/config/qwen3_1.7b_32b.yml)"
    )
    parser.add_argument(
        "--aime-config",
        type=str,
        default="skythought_evals/tasks/aime/aime25.yaml",
        help="Path to AIME25 task config (default: skythought_evals/tasks/aime/aime25.yaml)"
    )

    # Benchmark parameters
    parser.add_argument(
        "--num-problems",
        type=int,
        default=5,
        help="Number of AIME25 problems to test (default: 5)"
    )
    parser.add_argument(
        "--num-runs",
        type=int,
        default=5,
        help="Number of runs per problem (default: 5)"
    )
    parser.add_argument(
        "--test-mode",
        type=str,
        default="both",
        choices=["large", "spec", "both"],
        help="Which test to run: 'large' (large model only), 'spec' (speculative thinking only), or 'both' (default: both)"
    )
    parser.add_argument(
        "--use-2gpu",
        action="store_true",
        help="Use 2-GPU optimized version (loads both models in same process, avoids Ray actor conflicts)"
    )

    # Sampling parameters
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.6,
        help="Sampling temperature (default: 0.6)"
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.95,
        help="Nucleus sampling parameter (default: 0.95)"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=20,
        help="Top-k sampling parameter (default: 20)"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=32768,
        help="Maximum tokens to generate (default: 8192)"
    )

    args = parser.parse_args()

    # Verify config file exists
    if not os.path.exists(args.config):
        print(f"Error: Config file not found at {args.config}")
        print("Please ensure the config file exists.")
        sys.exit(1)

    if not os.path.exists(args.aime_config):
        print(f"Error: AIME config file not found at {args.aime_config}")
        print("Please ensure the AIME config file exists.")
        sys.exit(1)

    # Print configuration
    print("Configuration:")
    print(f"  Model config: {args.config}")
    print(f"  AIME config: {args.aime_config}")
    print(f"  Number of problems: {args.num_problems}")
    print(f"  Runs per problem: {args.num_runs}")
    print(f"  Test mode: {args.test_mode}")
    print(f"  Use 2-GPU mode: {args.use_2gpu}")
    print(f"  Temperature: {args.temperature}")
    print(f"  Top-p: {args.top_p}")
    print(f"  Top-k: {args.top_k}")
    print(f"  Max tokens: {args.max_tokens}")
    print()

    # Run benchmark
    benchmark = SpeedupBenchmark(
        config_path=args.config,
        num_problems=args.num_problems,
        num_runs=args.num_runs,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        max_tokens=args.max_tokens,
        test_mode=args.test_mode,
        use_2gpu=args.use_2gpu,
        aime_config_path=args.aime_config
    )

    benchmark.run()


if __name__ == "__main__":
    main()
