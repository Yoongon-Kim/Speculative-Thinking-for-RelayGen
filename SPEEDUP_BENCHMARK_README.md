# Speculative Thinking Speedup Benchmark

This benchmark measures the performance improvement of using speculative thinking (Qwen3-1.7B + Qwen3-32B) compared to using the large model (Qwen3-32B) alone.

## Overview

The benchmark tests both approaches on the first 5 problems from the AIME25 dataset, running each problem 5 times to get reliable statistics.

### Metrics Measured

1. **Latency (Wall Clock Time)**: Total time taken to generate the solution
2. **Token Throughput**: Tokens generated per second
3. **Large Model Token Ratio**: Percentage of tokens generated by the large model
4. **Total Tokens Generated**: Total number of tokens in the output

### Key Features

- Tests on AIME25 mathematical reasoning problems
- Runs multiple iterations for statistical reliability
- Handles 2-GPU constraint by loading both models simultaneously
- Provides detailed statistics and speedup analysis
- Saves results to JSON for further analysis

## Requirements

### Hardware
- 2 GPUs with sufficient VRAM for Qwen3-32B and Qwen3-1.7B
  - Recommended: 2x A100 40GB or similar

### Software
```bash
pip install torch transformers vllm datasets ray numpy
```

## Usage

### Basic Usage

```bash
python spec_thinking_speedup_test.py
```

This will use default parameters:
- 5 problems from AIME25 dataset
- 5 runs per problem
- Temperature: 0.6, Top-p: 0.95, Top-k: 20
- Max tokens: 32768
- Prompt template from `skythought_evals/tasks/aime/aime25.yaml`

### Command-Line Arguments

The benchmark supports full customization via command-line arguments:

```bash
python spec_thinking_speedup_test.py --help
```

**Available Arguments:**

```bash
# Model configuration
--config PATH                    Path to speculative thinking config
                                (default: speculative/config/qwen3_1.7b_32b.yml)

--aime-config PATH              Path to AIME25 task config with prompt template
                                (default: skythought_evals/tasks/aime/aime25.yaml)

# Benchmark parameters
--num-problems N                Number of problems to test (default: 5)
--num-runs N                    Number of runs per problem (default: 5)
--test-mode MODE                Which test to run: 'large', 'spec', or 'both' (default: both)

# Sampling parameters
--temperature FLOAT             Sampling temperature (default: 0.6)
--top-p FLOAT                   Nucleus sampling parameter (default: 0.95)
--top-k INT                     Top-k sampling parameter (default: 20)
--max-tokens INT                Maximum tokens to generate (default: 32768)
```

**Examples:**

```bash
# Test 10 problems with 3 runs each
python spec_thinking_speedup_test.py --num-problems 10 --num-runs 3

# Use different sampling parameters
python spec_thinking_speedup_test.py --temperature 0.7 --top-p 0.9 --top-k 50

# Use custom config files
python spec_thinking_speedup_test.py --config my_config.yml --aime-config my_aime.yaml

# Combine multiple parameters
python spec_thinking_speedup_test.py \
    --num-problems 3 \
    --num-runs 10 \
    --temperature 0.8 \
    --top-k 30

# Use custom max tokens
python spec_thinking_speedup_test.py --max-tokens 16384

# Run only large model test (useful to avoid OOM)
python spec_thinking_speedup_test.py --test-mode large

# Run only speculative thinking test
python spec_thinking_speedup_test.py --test-mode spec
```

### Configuration File

The benchmark uses the configuration file at `speculative/config/qwen3_1.7b_32b.yml`, which specifies:

- **Target Model**: Qwen3-32B (2 GPUs)
- **Speculative Model**: Qwen3-1.7B (1 GPU)
- **Speculative Thinking Parameters**: trigger tokens, validation keywords, etc.

## Output

### Console Output

The benchmark provides real-time progress updates and a final summary:

```
================================================================================
BENCHMARK RESULTS SUMMARY
================================================================================

1. LATENCY (seconds)
--------------------------------------------------------------------------------
Metric               Large Model          Spec Thinking        Speedup
--------------------------------------------------------------------------------
Mean                           45.32s              28.15s               1.61x
Median                         44.87s              27.92s               1.61x
...

2. THROUGHPUT (tokens/second)
--------------------------------------------------------------------------------
Metric               Large Model          Spec Thinking        Improvement
--------------------------------------------------------------------------------
Mean                        125.43 t/s          201.87 t/s               1.61x
...

3. TOKEN GENERATION
--------------------------------------------------------------------------------
...

4. LARGE MODEL USAGE RATIO
--------------------------------------------------------------------------------
Metric               Large Model          Spec Thinking
--------------------------------------------------------------------------------
Mean Ratio                    100.0%               45.2%
...

================================================================================
OVERALL SPEEDUP: 1.61x
THROUGHPUT IMPROVEMENT: 1.61x
LARGE MODEL TOKEN REDUCTION: 54.8%
================================================================================
```

### JSON Output

Detailed results are saved to `benchmark_results/speedup_benchmark_TIMESTAMP.json`:

```json
{
  "timestamp": "20260105_143022",
  "config": {
    "num_problems": 5,
    "num_runs": 5,
    "target_model": "Qwen/Qwen3-32B",
    "speculative_model": "Qwen/Qwen3-1.7B"
  },
  "large_model": {
    "results": { ... },
    "statistics": { ... }
  },
  "speculative_thinking": {
    "results": { ... },
    "statistics": { ... }
  },
  "comparison": {
    "speedup": 1.61,
    "throughput_improvement": 1.61,
    "large_model_reduction": 0.548
  }
}
```

## How It Works

### Large Model Only Test

1. Initializes Qwen3-32B on 2 GPUs
2. For each problem:
   - Runs 5 generations with the same parameters
   - Measures latency and token counts
3. Calculates statistics across all runs

### Speculative Thinking Test

1. Initializes both models:
   - Qwen3-32B on 2 GPUs (target model)
   - Qwen3-1.7B on 1 GPU (speculative model)
2. For each problem:
   - Runs 5 generations using speculative thinking
   - Tracks when the large model is called for validation
   - Measures latency, tokens, and large model usage
3. Calculates statistics across all runs

### GPU Memory Management

The benchmark carefully manages GPU memory with a 2-GPU constraint:

- **Large Model Only**: Uses 2 GPUs with `gpu_memory_utilization=0.9`
- **Speculative Thinking**:
  - Target model (Qwen3-32B): 2 GPUs, `gpu_memory_utilization=0.9`
  - Speculative model (Qwen3-1.7B): 1 GPU (tensor parallel), `gpu_memory_utilization=0.2`

The models are loaded sequentially, with proper cleanup between tests using Ray shutdown.

## Understanding the Results

### Speedup

The speedup metric indicates how much faster speculative thinking is compared to the large model alone:
- **Speedup > 1.0**: Speculative thinking is faster
- **Speedup = 1.0**: No difference
- **Speedup < 1.0**: Large model alone is faster

### Large Model Token Ratio

This metric shows what percentage of tokens were generated by the large model in speculative thinking:
- **Lower is better**: More tokens generated by the small model
- Typical range: 30-60%
- This directly correlates with speedup potential

### Throughput Improvement

Tokens per second improvement shows the raw generation speed increase:
- Directly related to speedup
- More relevant than latency for comparing model efficiency

## Troubleshooting

### Out of Memory Errors

The benchmark now includes several strategies to handle OOM errors:

**1. Run Tests Separately (Recommended)**
```bash
# Run large model test first
python spec_thinking_speedup_test.py --test-mode large

# Then run speculative thinking test
python spec_thinking_speedup_test.py --test-mode spec
```

This avoids loading both model configurations in the same process.

**2. Automatic Cleanup (for --test-mode both)**
When running both tests, the script:
- Shuts down Ray after the large model test
- Runs garbage collection
- Clears CUDA cache
- Waits 5 seconds for resources to be freed
- Then loads the speculative thinking models

**3. Additional Options**
If still encountering OOM:
1. Reduce `--num-problems` or `--num-runs`
2. Adjust `gpu_memory_utilization` in [speculative/speculative_vllm.py](speculative/speculative_vllm.py#L22)
3. Reduce `--max-tokens`
4. Ensure no other processes are using the GPUs

### Ray Initialization Errors

If Ray fails to initialize:
```bash
ray stop  # Stop any existing Ray processes
python spec_thinking_speedup_test.py
```

### Dataset Loading Issues

If the AIME25 dataset fails to load:
- Check internet connection (downloads from HuggingFace Hub)
- Verify HuggingFace datasets library is installed
- Try manually downloading: `huggingface-cli download yentinglin/aime_2025`

## Advanced Usage

### Testing Different Model Combinations

To test different model pairs, create a new config file in `speculative/config/`:

```yaml
mode: "vllm"
target_model_name: "Qwen/Qwen3-32B"
speculative_model_name: "Qwen/Qwen3-7B"  # Different speculative model
target_model_gpu: 2
speculative_model_gpu: 1
# ... rest of config
```

Then modify `config_path` in the script.

### Analyzing Results

The JSON output can be loaded and analyzed with Python:

```python
import json
import pandas as pd

with open('benchmark_results/speedup_benchmark_20260105_143022.json') as f:
    results = json.load(f)

# Extract statistics
large_latency = results['large_model']['statistics']['latency']['mean']
spec_latency = results['speculative_thinking']['statistics']['latency']['mean']
speedup = results['comparison']['speedup']

print(f"Speedup: {speedup:.2f}x")
```

## Expected Performance

Based on typical speculative decoding performance:
- **Speedup**: 1.3x - 2.0x
- **Large Model Token Ratio**: 30% - 60%
- **Throughput Improvement**: 1.3x - 2.0x

Actual performance depends on:
- Problem complexity
- Model agreement rate
- Trigger token frequency
- GPU hardware
